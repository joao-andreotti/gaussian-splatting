{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_splatting.colmap import parse_cameras, parse_images, parse_points3d, clean_text\n",
    "from gaussian_splatting.dataset import View, Cloud\n",
    "import diff_gaussian_rasterization as dg\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing colmap output\n",
    "with open(f\"../data/{dataset}/cameras.txt\", \"r\")  as f:\n",
    "    cameras = parse_cameras(clean_text(f.readlines()))\n",
    "\n",
    "with open(f\"../data/{dataset}/points3D.txt\", \"r\")  as f:\n",
    "    points3d = parse_points3d(clean_text(f.readlines()))\n",
    "\n",
    "with open(f\"../data/{dataset}/images.txt\", \"r\")  as f:\n",
    "    images = parse_images(clean_text(f.readlines()), cameras, points3d)\n",
    "\n",
    "# loading original images as nd arrays\n",
    "original_images = {}\n",
    "for image in images.values():\n",
    "    im = cv2.imread(f\"../data/{dataset}/images/{image.name}\")[:, :, ::-1] / 256\n",
    "    original_images[image.idx] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the point cloud\n",
    "cloud = Cloud(points3d.values())\n",
    "cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating some views\n",
    "views = [View(image) for image in images.values()]\n",
    "views[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian model from view\n",
    "def create_model(view: View):\n",
    "    configuration = dg.GaussianRasterizationSettings(\n",
    "        image_height=view.height,\n",
    "        image_width=view.width,\n",
    "        tanfovx=np.tan(view.fovx/2),\n",
    "        tanfovy=np.tan(view.fovy/2),\n",
    "        bg=torch.tensor([0, 0, 0], dtype=torch.float, device=\"cuda\"),\n",
    "        scale_modifier=1.,\n",
    "        viewmatrix=torch.tensor(view.viewmatrix(False).T, dtype=torch.float, device=\"cuda\"),\n",
    "        projmatrix=torch.tensor(view.viewmatrix(False).T @ view.projmatrix(znear=1).T, dtype=torch.float, device=\"cuda\"),\n",
    "        sh_degree=0,\n",
    "        campos=torch.tensor(view.position, dtype=torch.float, device=\"cuda\"),\n",
    "        prefiltered=False,\n",
    "        debug=False\n",
    "    )\n",
    "    return dg.GaussianRasterizer(configuration)\n",
    "\n",
    "models: list[tuple[dg.GaussianRasterizer, View]] = [(create_model(view), view) for view in views]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test splitting\n",
    "random.seed(42)\n",
    "random.shuffle(models)\n",
    "train_split = 0.7\n",
    "dataset_size = len(models)\n",
    "train = models[:int(dataset_size*train_split)]\n",
    "test = models[int(dataset_size*train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters from point cloud\n",
    "number_points = len(cloud.points)\n",
    "data = dict(\n",
    "    means3D=torch.tensor(cloud.points_positions(), dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    "    means2D=torch.zeros((number_points, 3), dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    "    shs=torch.tensor(cloud.points_colors(), dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    "    opacities=torch.ones([number_points, 1], dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    "    scales=torch.tensor([[.01, .01, .01]] * number_points, dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    "    rotations=torch.tensor([[1, 0, 0, 0]] * number_points, dtype=torch.float, device=\"cuda\", requires_grad=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model, view):\n",
    "    _img, _ = model(**data)\n",
    "    npimg = _img.cpu().detach().numpy().transpose([1, 2, 0])\n",
    "    orig_img = original_images[view.image.idx]\n",
    "    plt.imshow(np.hstack([npimg, orig_img]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(*test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_reconstruction_scaling_factor = 1.6 # \\phi\n",
    "opacity_threshold = 0.005\n",
    "positional_threshold = 2e-4 # \\tau_pos\n",
    "reconstruction_threshold = 0.05\n",
    "\n",
    "def quaternion_to_rotation_matrix(quat):\n",
    "    \"\"\" Convert a quaternion into a rotation matrix. \"\"\"\n",
    "    q0, q1, q2, q3 = quat.unbind(-1)\n",
    "    rot_matrix = torch.stack([\n",
    "        1 - 2*q2*q2 - 2*q3*q3, 2*q1*q2 - 2*q3*q0, 2*q1*q3 + 2*q2*q0,\n",
    "        2*q1*q2 + 2*q3*q0, 1 - 2*q1*q1 - 2*q3*q3, 2*q2*q3 - 2*q1*q0,\n",
    "        2*q1*q3 - 2*q2*q0, 2*q2*q3 + 2*q1*q0, 1 - 2*q1*q1 - 2*q2*q2\n",
    "    ], dim=-1).reshape(-1, 3, 3)\n",
    "    return rot_matrix\n",
    "\n",
    "def rotate_points(scales, rotations, points):\n",
    "    scaled = scales * points\n",
    "    quaternions = rotations / rotations.norm(dim=1, keepdim=True)\n",
    "    rot_matrices = quaternion_to_rotation_matrix(quaternions)\n",
    "    rotated_points = torch.bmm(rot_matrices, scaled.unsqueeze(-1)).squeeze(-1)\n",
    "    return rotated_points\n",
    "\n",
    "def adaptive_density_control(means3D, means2D, shs, opacities, scales, rotations):\n",
    "    almost_transparent = (opacities < opacity_threshold).squeeze()\n",
    "    needs_adapting = means2D.grad.square().sum(dim=1).sqrt() > positional_threshold\n",
    "    under_reconstructed = needs_adapting & (scales.square().sum(dim=1).sqrt() < reconstruction_threshold)\n",
    "    over_reconstructed = needs_adapting & (scales.square().sum(dim=1).sqrt() >= reconstruction_threshold)\n",
    "\n",
    "    print(f\"out of {means3D.shape[0]} gaussians, {needs_adapting.sum()} need adapting\")\n",
    "    print(f\"out of those {needs_adapting.sum()}, {under_reconstructed.sum()} are under-reconstructed\")\n",
    "    print(f\"out of those {needs_adapting.sum()}, {over_reconstructed.sum()} are over-reconstructed\")\n",
    "\n",
    "    filter_points = (\n",
    "        almost_transparent|\n",
    "        needs_adapting\n",
    "    )\n",
    "\n",
    "    # lets double gaussians that are under_reconstructed\n",
    "    under_reconstructed_means3D = torch.vstack([\n",
    "        means3D[under_reconstructed], \n",
    "        means3D[under_reconstructed] + means3D.grad[under_reconstructed]\n",
    "    ])\n",
    "    under_reconstructed_means2D = torch.vstack([means2D[under_reconstructed]] * 2)\n",
    "    under_reconstructed_scales = torch.vstack([scales[under_reconstructed]] * 2)\n",
    "    under_reconstructed_opacities = torch.vstack([opacities[under_reconstructed]] * 2)\n",
    "    under_reconstructed_shs = torch.vstack([shs[under_reconstructed]] * 2)\n",
    "    under_reconstructed_rotations = torch.vstack([rotations[under_reconstructed]] * 2)\n",
    "\n",
    "    # lets double gaussians that are very over_reconstructed\n",
    "\n",
    "    # sample from gaussian (0, 1 distribution) \n",
    "    first_set_points = torch.normal(0, 1, size=(over_reconstructed.sum(), 3)).cuda()\n",
    "    first_rotated = rotate_points(scales[over_reconstructed], rotations[over_reconstructed], first_set_points)\n",
    "\n",
    "    second_set_points = torch.normal(0, 1, size=(over_reconstructed.sum(), 3)).cuda()\n",
    "    second_rotated = rotate_points(scales[over_reconstructed], rotations[over_reconstructed], second_set_points)\n",
    "    \n",
    "    over_reconstructed_means3D = torch.vstack([\n",
    "        means3D[over_reconstructed] + first_rotated, \n",
    "        means3D[over_reconstructed] + second_rotated\n",
    "    ])\n",
    "    over_reconstructed_means2D = torch.vstack([means2D[over_reconstructed]] * 2)\n",
    "    over_reconstructed_scales = torch.vstack([scales[over_reconstructed]] * 2) / over_reconstruction_scaling_factor\n",
    "    over_reconstructed_opacities = torch.vstack([opacities[over_reconstructed]] * 2)\n",
    "    over_reconstructed_shs = torch.vstack([shs[over_reconstructed]] * 2)\n",
    "    over_reconstructed_rotations = torch.vstack([rotations[over_reconstructed]] * 2)\n",
    "\n",
    "    # stacking everything\n",
    "    new_means3D = torch.vstack([    means3D[~filter_points],        over_reconstructed_means3D,     under_reconstructed_means3D])\n",
    "    new_means2D = torch.vstack([    means2D[~filter_points],        over_reconstructed_means2D,     under_reconstructed_means2D])\n",
    "    new_scales = torch.vstack([     scales[~filter_points],         over_reconstructed_scales,      under_reconstructed_scales])\n",
    "    new_opacities = torch.vstack([  opacities[~filter_points],      over_reconstructed_opacities,   under_reconstructed_opacities])\n",
    "    new_shs = torch.vstack([        shs[~filter_points],            over_reconstructed_shs,         under_reconstructed_shs])\n",
    "    new_rotations = torch.vstack([  rotations[~filter_points],      over_reconstructed_rotations,   under_reconstructed_rotations])\n",
    "    \n",
    "    return dict(\n",
    "        means3D=new_means3D.detach().requires_grad_(),\n",
    "        means2D=new_means2D.detach().requires_grad_(),\n",
    "        scales=new_scales.detach().requires_grad_(),\n",
    "        opacities=new_opacities.detach().requires_grad_(),\n",
    "        shs=new_shs.detach().requires_grad_(),\n",
    "        rotations=new_rotations.detach().requires_grad_(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_splatting.loss import DSSIM\n",
    "\n",
    "ssim_loss = DSSIM\n",
    "l1_loss = lambda img1, img2: torch.abs(img1 - img2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting images in the gpu\n",
    "original_images_gpu = {\n",
    "    key: torch.tensor(value, dtype=torch.float, device=\"cuda\").permute(2, 0, 1)\n",
    "    for key, value in original_images.items()\n",
    "}\n",
    "\n",
    "images = [*original_images_gpu.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "_lambda = 0.2\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = lambda img1, img2: (1 - _lambda) * l1_loss(img1, img2) + _lambda * ssim_loss(img1, img2)\n",
    "# initial optimizer\n",
    "create_optimizer = lambda variables: torch.optim.Adam(variables, learning_rate)\n",
    "optimizer = create_optimizer(data.values())\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(1, 100):\n",
    "    s = time.perf_counter_ns()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    for model, view in train:\n",
    "        original_img = original_images_gpu[view.image.idx]\n",
    "        out_img, idx = model(**data)\n",
    "        loss = criterion(original_img, out_img)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= len(train)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for model, view in test:\n",
    "        with torch.no_grad():\n",
    "            original_img = original_images_gpu[view.image.idx]\n",
    "            out_img, d = model(**data)\n",
    "            loss = criterion(original_img, out_img)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(test)\n",
    "    test_loss.append(epoch_loss)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        data = adaptive_density_control(**data)\n",
    "        optimizer = create_optimizer(data.values())\n",
    "\n",
    "    print(f\"{epoch:4} loss: {epoch_loss:.6f}\")\n",
    "    print(f\"total train loop time {(time.perf_counter_ns() - s)*1e-6}ms\")\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test:\n",
    "    compare(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating my ssim loss...\n",
    "image = original_images_gpu[50].clone()\n",
    "approx = torch.rand_like(image, dtype=torch.float, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "plt.imshow(image.clone().cpu().detach().numpy().transpose([1, 2, 0]))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(approx.clone().cpu().detach().numpy().transpose([1, 2, 0]))\n",
    "plt.show()\n",
    "\n",
    "_lambda = 1\n",
    "criterion = lambda img1, img2: (1 - _lambda) * l1_loss(img1, img2) + _lambda * ssim_loss(img1, img2)\n",
    "optimizer = torch.optim.Adam([approx], 0.01)\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(image, approx)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss}\")\n",
    "\n",
    "plt.imshow(approx.clone().cpu().detach().numpy().transpose([1, 2, 0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysing execution bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model, view = models[0]\n",
    "original_img = original_images_gpu[view.image.idx]\n",
    "out_img, d = model(**data)\n",
    "\n",
    "N = 100\n",
    "\n",
    "s = time.perf_counter_ns()\n",
    "for _ in range(N):\n",
    "    l1_loss(original_img, out_img)\n",
    "l1_time = time.perf_counter_ns() - s\n",
    "\n",
    "s = time.perf_counter_ns()\n",
    "for _ in range(N):\n",
    "    ssim_loss(original_img, out_img)\n",
    "ssim_time = time.perf_counter_ns() - s\n",
    "\n",
    "print(f\"f1 loss took on average {l1_time/N*1e-6} ms\")\n",
    "print(f\"ssim loss took on average {ssim_time/N*1e-6} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
